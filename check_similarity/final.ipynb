{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d7f7eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìò Extracting text from: 1706.03762v7.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Extraction complete! Saved to: C:\\Users\\BHUVANA VIJAYA\\OneDrive\\Documents\\GitHub\\assignment_2_AI\\input_text.json\n"
     ]
    }
   ],
   "source": [
    "#text extract\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from pdfminer.high_level import extract_text\n",
    "\n",
    "# === Custom Input ===\n",
    "pdf_path = input(\"Enter the path to your PDF file: \").strip()\n",
    "\n",
    "pdf_file = Path(pdf_path)\n",
    "if not pdf_file.exists():\n",
    "    raise FileNotFoundError(f\"‚ùå PDF not found at: {pdf_file}\")\n",
    "\n",
    "# === Clean text function ===\n",
    "def clean_text(text):\n",
    "    \"\"\"Cleans text by removing unwanted characters and normalizing whitespace.\"\"\"\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'[^A-Za-z0-9.,:;!?()\\-\\'\"\\s]', '', text)\n",
    "    return text.strip().lower()\n",
    "\n",
    "# === Extract and clean text ===\n",
    "print(f\"üìò Extracting text from: {pdf_file.name}\")\n",
    "try:\n",
    "    raw_text = extract_text(str(pdf_file))\n",
    "    cleaned_text = clean_text(raw_text)\n",
    "    if len(cleaned_text) < 50:\n",
    "        print(\"‚ö†Ô∏è Warning: extracted text is too short, may be scanned or image-based PDF.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error extracting text: {e}\")\n",
    "    cleaned_text = \"\"\n",
    "\n",
    "# === Save to JSON ===\n",
    "output_json = {\n",
    "    \"filename\": pdf_file.name,\n",
    "    \"text\": cleaned_text\n",
    "}\n",
    "\n",
    "output_path = pdf_file.parent / \"input_text.json\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(output_json, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Extraction complete! Saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b814908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Creating TF-IDF vectors...\n",
      "\n",
      "üìä Cosine Similarity Scores:\n",
      "Amit Saxena: 0.0709\n",
      "Amita Jain: 0.0729\n",
      "Animesh Chaturvedi: 0.1495\n",
      "Ankita Jain: 0.1002\n",
      "Arun Chauhan: 0.2568\n",
      "Aruna Malapati: 0.1819\n",
      "Aruna Tiwari: 0.1594\n",
      "Barsha Mitra: 0.0832\n",
      "Bhanukiran Perabathini: 0.0825\n",
      "Bharghava Rajaram: 0.0632\n",
      "Deepak K T: 0.1452\n",
      "Devendra K Tayal: 0.1618\n",
      "Dilip Singh Sisodia: 0.1679\n",
      "dipanjan roy: 0.0699\n",
      "Dipti Mishra: 0.1860\n",
      "Dr. Ashish Jain: 0.1624\n",
      "Dr. Shikha Mehta: 0.1336\n",
      "Dr.Manpreet Kaur: 0.1284\n",
      "Dr.Rohit Beniwal: 0.1068\n",
      "Dr.Ruchi Mittal: 0.1270\n",
      "esha baidya kayal: 0.0775\n",
      "Geeta Rani: 0.2512\n",
      "Himanee Bansal: 0.1106\n",
      "Himanshu Mittal: 0.1361\n",
      "J. Balasubramaniam: 0.0489\n",
      "Jagdish Bansal: 0.0996\n",
      "Jayasri D: 0.0437\n",
      "Jian Wang: 0.1429\n",
      "K.V. Sambasivarao: 0.1149\n",
      "Kastuv Nag: 0.1228\n",
      "Khaldoon Dhou: 0.0652\n",
      "Krishna Asawa: 0.1659\n",
      "Mala Saraswat: 0.1570\n",
      "Manju_JaypeeTech: 0.0612\n",
      "Manoranjan Mohanty: 0.1327\n",
      "Minni Jain: 0.2511\n",
      "Mukesh Prasad: 0.1595\n",
      "Navneet Pratap Singh: 0.1170\n",
      "Nikhil Tripathi: 0.0854\n",
      "Nishchal K. Verma: 0.1199\n",
      "Om Prakash Patel: 0.1309\n",
      "OmPrakash Kaiwartya: 0.1027\n",
      "Pabitra Mitra: 0.0280\n",
      "Payal Khurana Batra: 0.1533\n",
      "Pinaki Chakraborty: 0.1343\n",
      "Prakash Chandra Sharma: 0.0709\n",
      "Prof. B Subudhi: 0.1033\n",
      "Rama Murthy Garimella: 0.1375\n",
      "Ramakrishan Maheshwari: 0.0297\n",
      "Ramalinga Swamy Cheruku: 0.2543\n",
      "ruchi sharma: 0.1057\n",
      "Rudresh dwivedi: 0.1248\n",
      "Sambit Bakshi: 0.1739\n",
      "Sanatan Sukhija: 0.1433\n",
      "Shikha Gupta: 0.2347\n",
      "Sowmini Devi: 0.0733\n",
      "Sreedhar Madichetty: 0.0669\n",
      "Srishti Sharma: 0.0855\n",
      "Sulabh Tyagi: 0.0686\n",
      "Sunny Rai: 0.0615\n",
      "sushma hans: 0.1153\n",
      "Tandra Pal: 0.0875\n",
      "Tingwen Huang: 0.1236\n",
      "Udit Satija: 0.0689\n",
      "V. Ravi: 0.1778\n",
      "Vaishali Soni: 0.0513\n",
      "Venkata Dilip Kumar: 0.1346\n",
      "Venkata Rajesh Kumar Tavva: 0.0761\n",
      "Vidhi Khanduja: 0.0728\n",
      "Yayati Gupta: 0.1208\n",
      "Yew-Soon Ong: 0.1467\n",
      "\n",
      "üèÜ Author(s) with Highest Similarity:\n",
      "‚û°Ô∏è Arun Chauhan (Score: 0.2568)\n",
      "\n",
      "‚úÖ Similarity comparison complete!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from pathlib import Path\n",
    "\n",
    "# === Paths ===\n",
    "input_json_path = Path(r\"C:\\Users\\BHUVANA VIJAYA\\OneDrive\\Documents\\GitHub\\assignment_2_AI\\input_text.json\")\n",
    "authors_json_path = Path(r\"C:\\Users\\BHUVANA VIJAYA\\OneDrive\\Documents\\GitHub\\assignment_2_AI\\extracted_data\\author_texts_pdfminer.json\")\n",
    "\n",
    "# === Load input text ===\n",
    "with open(input_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    input_data = json.load(f)\n",
    "input_text = input_data.get(\"text\", \"\").strip()\n",
    "\n",
    "if not input_text:\n",
    "    raise ValueError(\"‚ùå No text found in input_text.json\")\n",
    "\n",
    "# === Load authors dataset ===\n",
    "with open(authors_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    authors_json = json.load(f)\n",
    "\n",
    "if not authors_json:\n",
    "    raise ValueError(\"‚ùå author_texts_pdfminer.json is empty or invalid.\")\n",
    "\n",
    "# === Combine all papers per author ===\n",
    "author_names = list(authors_json.keys())\n",
    "all_texts = [\" \".join(author_papers) for author_papers in authors_json.values()]\n",
    "\n",
    "# === TF-IDF Vectorization ===\n",
    "print(\"üîç Creating TF-IDF vectors...\")\n",
    "vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 2), max_features=10000)\n",
    "X = vectorizer.fit_transform(all_texts)\n",
    "query_vec = vectorizer.transform([input_text])\n",
    "\n",
    "# === Compute cosine similarity ===\n",
    "cosine_sim = cosine_similarity(query_vec, X)[0]\n",
    "\n",
    "# === Find highest similarity ===\n",
    "max_score = cosine_sim.max()\n",
    "best_indices = [i for i, s in enumerate(cosine_sim) if s == max_score]\n",
    "\n",
    "print(\"\\nüìä Cosine Similarity Scores:\")\n",
    "for author, score in zip(author_names, cosine_sim):\n",
    "    print(f\"{author}: {score:.4f}\")\n",
    "\n",
    "print(\"\\nüèÜ Author(s) with Highest Similarity:\")\n",
    "for idx in best_indices:\n",
    "    print(f\"‚û°Ô∏è {author_names[idx]} (Score: {cosine_sim[idx]:.4f})\")\n",
    "\n",
    "print(\"\\n‚úÖ Similarity comparison complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9ef12ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìò PDF Reference Extractor ‚Üí JSON (Clean Format)\n",
      "‚úÖ References extracted and saved to 'input_references.json'\n",
      "\n",
      "‚úÖ Found 46 reference entries.\n",
      "üìÑ First few references:\n",
      "\n",
      "1. Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450,...\n",
      "2. Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. CoRR, abs/1409.0473,...\n",
      "3. Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural machine translation architectures. CoRR, abs/1703.03906,...\n",
      "4. Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine reading. arXiv preprint arXiv:1601.06733,...\n",
      "5. Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. CoRR, abs/1406.1078,...\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "\n",
    "def extract_references(pdf_path, save_to_json=True):\n",
    "    \"\"\"Extract and clean the References section from a PDF.\"\"\"\n",
    "    if not os.path.exists(pdf_path):\n",
    "        print(f\"‚ùå File not found: {pdf_path}\")\n",
    "        return None\n",
    "\n",
    "    doc = fitz.open(pdf_path)\n",
    "    full_text = \"\"\n",
    "\n",
    "    # --- 1Ô∏è‚É£ Extract text from all pages ---\n",
    "    for page in doc:\n",
    "        text = page.get_text(\"text\")\n",
    "        full_text += text + \"\\n\"\n",
    "\n",
    "    # Normalize whitespace\n",
    "    full_text = re.sub(r'\\s+', ' ', full_text).strip()\n",
    "\n",
    "    # --- 2Ô∏è‚É£ Locate 'References' section ---\n",
    "    section_pattern = re.compile(\n",
    "        r'(references|bibliography|reference list|references and notes)[:\\s\\-]*',\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "    start_match = section_pattern.search(full_text)\n",
    "\n",
    "    if not start_match:\n",
    "        print(\"‚ö†Ô∏è No 'References' section found in the PDF.\")\n",
    "        return None\n",
    "\n",
    "    start_idx = start_match.end()\n",
    "    references_text = full_text[start_idx:].strip()\n",
    "\n",
    "    # --- 3Ô∏è‚É£ Stop before next section (Appendix, Acknowledgements, etc.) ---\n",
    "    end_match = re.search(\n",
    "        r'(appendix|acknowledg(e)?ments?|supplementary materials?)',\n",
    "        references_text,\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "    if end_match:\n",
    "        references_text = references_text[:end_match.start()].strip()\n",
    "\n",
    "    # --- 4Ô∏è‚É£ Split into individual reference entries ---\n",
    "    entries = re.split(r'\\s*(?:\\[\\d+\\]|\\d+\\.\\s+|‚Ä¢\\s+)\\s*', references_text)\n",
    "    entries = [e.strip() for e in entries if len(e.strip()) > 20]\n",
    "\n",
    "    # --- 5Ô∏è‚É£ Save clean JSON (no ids, no text objects) ---\n",
    "    if save_to_json:\n",
    "        output_file = \"input_references.json\"\n",
    "        data = {\n",
    "            \"pdf_file\": os.path.basename(pdf_path),\n",
    "            \"reference_count\": len(entries),\n",
    "            \"references\": entries\n",
    "        }\n",
    "\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "        print(f\"‚úÖ References extracted and saved to '{output_file}'\")\n",
    "\n",
    "    return entries\n",
    "\n",
    "\n",
    "# --- üîπ MAIN EXECUTION ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üìò PDF Reference Extractor ‚Üí JSON (Clean Format)\")\n",
    "    pdf_path = input(\"Enter the path to your PDF file: \").strip().strip('\"')\n",
    "\n",
    "    references = extract_references(pdf_path)\n",
    "\n",
    "    if references:\n",
    "        print(f\"\\n‚úÖ Found {len(references)} reference entries.\")\n",
    "        print(\"üìÑ First few references:\\n\")\n",
    "        for i, ref in enumerate(references[:5], 1):\n",
    "            print(f\"{i}. {ref[:250]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda7791d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find similarity between references is pending......"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1756752c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Keywords extraction complete for 1706.03762v7.pdf\n",
      "üìÅ Saved to: C:\\Users\\BHUVANA VIJAYA\\OneDrive\\Documents\\GitHub\\assignment_2_AI\\input_keywords.json\n",
      "üîë Top 10 Keywords: ['attention', 'model', 'models', 'sequence', 'arxiv', 'output', 'layer', 'transformer', 'neural', 'selfattention']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\BHUVANA\n",
      "[nltk_data]     VIJAYA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Download stopwords (only needed once)\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# === Paths ===\n",
    "input_json_path = Path(r\"C:\\Users\\BHUVANA VIJAYA\\OneDrive\\Documents\\GitHub\\assignment_2_AI\\input_text.json\")\n",
    "output_json_path = input_json_path.parent / \"input_keywords.json\"\n",
    "\n",
    "# === Load input text ===\n",
    "with open(input_json_path, 'r', encoding='utf-8') as f:\n",
    "    input_data = json.load(f)\n",
    "input_text = input_data.get(\"text\", \"\").strip()\n",
    "\n",
    "if not input_text:\n",
    "    raise ValueError(\"‚ùå No text found in input_text.json\")\n",
    "\n",
    "# === Preprocessing function ===\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    words = [word for word in text.split() if word not in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# === Keyword extraction function ===\n",
    "def extract_top_keywords(doc_text, top_n=50):\n",
    "    vectorizer = TfidfVectorizer(max_features=5000)\n",
    "    tfidf_matrix = vectorizer.fit_transform([doc_text])\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    sorted_nzs = np.argsort(tfidf_matrix.toarray()[0])[::-1][:top_n]\n",
    "    keywords = [feature_names[i] for i in sorted_nzs]\n",
    "    return keywords\n",
    "\n",
    "# === Process the input text ===\n",
    "clean_text = preprocess_text(input_text)\n",
    "keywords = extract_top_keywords(clean_text, top_n=50)\n",
    "\n",
    "# === Save to JSON ===\n",
    "output_data = {\n",
    "    \"filename\": input_data.get(\"filename\", \"unknown.pdf\"),\n",
    "    \"top_keywords\": keywords\n",
    "}\n",
    "\n",
    "with open(output_json_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(output_data, f, indent=4)\n",
    "\n",
    "print(f\"‚úÖ Keywords extraction complete for {input_data.get('filename', 'input file')}\")\n",
    "print(f\"üìÅ Saved to: {output_json_path}\")\n",
    "print(f\"üîë Top 10 Keywords: {keywords[:10]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29a653f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÜ Top Matching Authors by Keyword (Jaccard) Similarity:\n",
      "Arun Chauhan: 0.1640\n",
      "Ramalinga Swamy Cheruku: 0.1254\n",
      "Minni Jain: 0.1110\n",
      "Jian Wang: 0.1031\n",
      "Payal Khurana Batra: 0.1010\n",
      "Shikha Gupta: 0.1003\n",
      "Deepak K T: 0.0994\n",
      "Om Prakash Patel: 0.0956\n",
      "Aruna Tiwari: 0.0904\n",
      "Dipti Mishra: 0.0901\n",
      "\n",
      "üìÅ Similarity results saved to: C:\\Users\\BHUVANA VIJAYA\\OneDrive\\Documents\\GitHub\\assignment_2_AI\\keyword_similarity_results.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# === Load input paper keywords ===\n",
    "input_keywords_path = r\"C:\\Users\\BHUVANA VIJAYA\\OneDrive\\Documents\\GitHub\\assignment_2_AI\\input_keywords.json\"\n",
    "with open(input_keywords_path, 'r', encoding='utf-8') as f:\n",
    "    input_keywords = set(json.load(f)[\"top_keywords\"])\n",
    "\n",
    "# === Load the author keywords dataset ===\n",
    "author_keywords_path = r\"C:\\Users\\BHUVANA VIJAYA\\OneDrive\\Documents\\GitHub\\assignment_2_AI\\extracted_data\\authors_keywords.json\"\n",
    "with open(author_keywords_path, 'r', encoding='utf-8') as f:\n",
    "    author_data = json.load(f)\n",
    "\n",
    "# === Define Jaccard similarity ===\n",
    "def jaccard_similarity(set1, set2):\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    return intersection / union if union else 0\n",
    "\n",
    "# === Compute average Jaccard similarity per author ===\n",
    "author_scores = {}\n",
    "\n",
    "for author, papers_keywords in author_data.items():\n",
    "    if not papers_keywords:\n",
    "        continue\n",
    "\n",
    "    paper_scores = []\n",
    "    for keywords in papers_keywords:\n",
    "        if not keywords:\n",
    "            continue\n",
    "        score = jaccard_similarity(input_keywords, set(keywords))\n",
    "        paper_scores.append(score)\n",
    "\n",
    "    if paper_scores:\n",
    "        author_scores[author] = sum(paper_scores) / len(paper_scores)\n",
    "\n",
    "# === Sort authors by similarity score ===\n",
    "sorted_authors = sorted(author_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# === Display top authors ===\n",
    "print(\"üèÜ Top Matching Authors by Keyword (Jaccard) Similarity:\")\n",
    "for author, score in sorted_authors[:10]:\n",
    "    print(f\"{author}: {score:.4f}\")\n",
    "\n",
    "# === (Optional) Save results ===\n",
    "output_path = r\"C:\\Users\\BHUVANA VIJAYA\\OneDrive\\Documents\\GitHub\\assignment_2_AI\\keyword_similarity_results.json\"\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(sorted_authors, f, indent=4)\n",
    "\n",
    "print(f\"\\nüìÅ Similarity results saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ff2400",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2d5588",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

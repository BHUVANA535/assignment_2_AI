{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b1778fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\BHUVANA VIJAYA\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\BHUVANA VIJAYA\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\BHUVANA VIJAYA\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "461e6bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-22 22:55:51.154 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-22 22:55:51.362 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run c:\\Users\\BHUVANA VIJAYA\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-10-22 22:55:51.362 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-22 22:55:51.362 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-22 22:55:56.061 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-22 22:55:56.061 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-22 22:55:56.065 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-22 22:55:56.066 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-22 22:55:56.066 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-22 22:55:56.066 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-22 22:55:56.073 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-22 22:55:56.074 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-22 22:55:56.076 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import PyPDF2\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from itertools import chain\n",
    "\n",
    "# -----------------------\n",
    "# Load Pre-Extracted Data\n",
    "# -----------------------\n",
    "# Paths to your JSON files\n",
    "TEXT_JSON = r\"extracted_data/author_texts_pdfminer.json\"\n",
    "KEYWORDS_JSON = r\"extracted_data/authors_keywords.json\"\n",
    "REFERENCES_JSON = r\"extracted_data\\references_dataset.json\"\n",
    "\n",
    "with open(TEXT_JSON, 'r', encoding='utf-8') as f:\n",
    "    authors_texts = json.load(f)\n",
    "\n",
    "with open(KEYWORDS_JSON, 'r', encoding='utf-8') as f:\n",
    "    authors_keywords = json.load(f)\n",
    "\n",
    "with open(REFERENCES_JSON, 'r', encoding='utf-8') as f:\n",
    "    authors_references = json.load(f)\n",
    "\n",
    "# -----------------------\n",
    "# Initialize BERT Model\n",
    "# -----------------------\n",
    "st.info(\"Loading sentence transformer model (BERT)...\")\n",
    "bert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# -----------------------\n",
    "# Streamlit Interface\n",
    "# -----------------------\n",
    "st.title(\"Reviewer Recommendation System\")\n",
    "uploaded_file = st.file_uploader(\"Upload your research paper (PDF)\", type=\"pdf\")\n",
    "\n",
    "if uploaded_file:\n",
    "    # 1️⃣ Extract text from PDF\n",
    "    reader = PyPDF2.PdfReader(uploaded_file)\n",
    "    input_text = \"\"\n",
    "    for page in reader.pages:\n",
    "        input_text += page.extract_text() or \"\"\n",
    "\n",
    "    st.success(\"PDF uploaded successfully!\")\n",
    "\n",
    "    # -----------------------\n",
    "    # 2️⃣ Keyword Extraction (Simple approach: top-n frequent words)\n",
    "    # You can replace with RAKE / KeyBERT for better extraction\n",
    "    # -----------------------\n",
    "    import re\n",
    "    from collections import Counter\n",
    "\n",
    "    words = re.findall(r'\\b\\w+\\b', input_text.lower())\n",
    "    word_counts = Counter(words)\n",
    "    input_keywords = [w for w, _ in word_counts.most_common(20)]  # top 20 words\n",
    "\n",
    "    # -----------------------\n",
    "    # 3️⃣ Reference Extraction (Assuming references in last 10% of PDF)\n",
    "    # -----------------------\n",
    "    total_pages = len(reader.pages)\n",
    "    ref_text = \"\"\n",
    "    for page in reader.pages[int(0.9 * total_pages):]:\n",
    "        ref_text += page.extract_text() or \"\"\n",
    "    input_references = re.findall(r'\\b\\d{4}\\b|[A-Z][a-z]+ et al\\.', ref_text)  # crude extraction\n",
    "\n",
    "    # -----------------------\n",
    "    # 4️⃣ Topic Modeling (LDA)\n",
    "    # -----------------------\n",
    "    count_vectorizer = CountVectorizer(stop_words='english')\n",
    "    input_count = count_vectorizer.fit_transform([input_text])\n",
    "    lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "    input_topic_dist = lda.fit_transform(input_count)[0]\n",
    "\n",
    "    # -----------------------\n",
    "    # 5️⃣ BERT Embedding\n",
    "    # -----------------------\n",
    "    input_emb = bert_model.encode([input_text])[0]\n",
    "\n",
    "    # -----------------------\n",
    "    # 6️⃣ Compute Similarities per Author\n",
    "    # -----------------------\n",
    "    final_scores = {}\n",
    "\n",
    "    for author, papers in authors_texts.items():\n",
    "        # ---- Text Similarity (TF-IDF) ----\n",
    "        corpus = papers + [input_text]\n",
    "        tfidf_vec = TfidfVectorizer(stop_words='english')\n",
    "        tfidf_matrix = tfidf_vec.fit_transform(corpus)\n",
    "        text_sims = cosine_similarity(tfidf_matrix[-1], tfidf_matrix[:-1])\n",
    "        text_sim_score = np.mean(text_sims)\n",
    "\n",
    "        # ---- Keyword Similarity (Jaccard) ----\n",
    "        author_keywords_flat = list(chain.from_iterable(authors_keywords.get(author, [])))\n",
    "        intersection = len(set(input_keywords) & set(author_keywords_flat))\n",
    "        union = len(set(input_keywords) | set(author_keywords_flat))\n",
    "        keyword_score = intersection / union if union > 0 else 0\n",
    "\n",
    "        # ---- Reference Similarity ----\n",
    "        author_refs_flat = list(chain.from_iterable(authors_references.get(author, {}).values()))\n",
    "        intersection = len(set(input_references) & set(author_refs_flat))\n",
    "        union = len(set(input_references) | set(author_refs_flat))\n",
    "        ref_score = intersection / union if union > 0 else 0\n",
    "\n",
    "        # ---- Topic Similarity ----\n",
    "        # Average topic distribution of author papers (LDA on each paper)\n",
    "        author_topic_dists = []\n",
    "        for paper_text in papers:\n",
    "            count_vec = count_vectorizer.fit_transform([paper_text])\n",
    "            topic_dist = lda.transform(count_vec)[0]\n",
    "            author_topic_dists.append(topic_dist)\n",
    "        if author_topic_dists:\n",
    "            avg_author_topic = np.mean(author_topic_dists, axis=0)\n",
    "            topic_score = cosine_similarity([input_topic_dist], [avg_author_topic])[0][0]\n",
    "        else:\n",
    "            topic_score = 0\n",
    "\n",
    "        # ---- BERT Embedding Similarity ----\n",
    "        author_embs = [bert_model.encode([p])[0] for p in papers]\n",
    "        bert_sims = [cosine_similarity([input_emb], [emb])[0][0] for emb in author_embs]\n",
    "        bert_score = np.mean(bert_sims) if bert_sims else 0\n",
    "\n",
    "        # ---- Aggregate Final Score ----\n",
    "        final_score = np.mean([text_sim_score, keyword_score, ref_score, topic_score, bert_score])\n",
    "        final_scores[author] = final_score\n",
    "\n",
    "    # -----------------------\n",
    "    # 7️⃣ Display Top-k Authors\n",
    "    # -----------------------\n",
    "    top_k = 5\n",
    "    sorted_authors = sorted(final_scores.items(), key=lambda x: x[1], reverse=True)[:top_k]\n",
    "\n",
    "    st.subheader(f\"Top {top_k} Recommended Reviewers\")\n",
    "    for i, (author, score) in enumerate(sorted_authors, 1):\n",
    "        st.write(f\"{i}. {author} - Score: {score:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

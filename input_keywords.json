{
    "filename": "1706.03762v7.pdf",
    "top_keywords": [
        "attention",
        "model",
        "models",
        "sequence",
        "arxiv",
        "output",
        "layer",
        "transformer",
        "neural",
        "selfattention",
        "training",
        "input",
        "translation",
        "layers",
        "positions",
        "encoder",
        "recurrent",
        "decoder",
        "used",
        "values",
        "two",
        "different",
        "learning",
        "machine",
        "preprint",
        "networks",
        "length",
        "dk",
        "table",
        "function",
        "position",
        "tasks",
        "also",
        "wsj",
        "bleu",
        "base",
        "heads",
        "results",
        "representations",
        "use",
        "using",
        "positional",
        "number",
        "network",
        "al",
        "big",
        "trained",
        "set",
        "tokens",
        "language"
    ]
}